{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéÆ Valorant Kill Predictor - Streamlined Google Colab Training\n",
        "\n",
        "This notebook trains your Valorant kill prediction model using advanced features.\n",
        "\n",
        "## üìã Streamlined Workflow:\n",
        "1. **Cell 1**: Install dependencies and imports\n",
        "2. **Cell 2**: PyTorch classes and utilities  \n",
        "3. **Cell 3**: Upload your database file\n",
        "4. **Cell 4**: Ultra-fast classes (required for advanced training)\n",
        "5. **Cell 5**: Advanced feature training (THE MAIN ONE!)\n",
        "6. **Cell 6**: Download trained model\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Cell 1: Install Dependencies and Imports\n",
        "\n",
        "# Install PyTorch if needed\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úÖ PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing PyTorch...\")\n",
        "    %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    import torch\n",
        "\n",
        "# All necessary imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import joblib\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import sqlite3\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime, timedelta\n",
        "import difflib\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üî• Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"‚úÖ All imports loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß† Cell 2: PyTorch Classes and Utilities\n",
        "\n",
        "class KillPredictionDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for kill prediction\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class KillPredictionNN(nn.Module):\n",
        "    \"\"\"Neural network for kill prediction\"\"\"\n",
        "    def __init__(self, input_size: int, hidden_sizes: List[int] = [128, 64, 32]):\n",
        "        super(KillPredictionNN, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_size, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.BatchNorm1d(hidden_size)\n",
        "            ])\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def check_database_schema(db_path):\n",
        "    \"\"\"Check if database has required tables\"\"\"\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"‚ùå Database file not found: {db_path}\")\n",
        "        return False\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "        tables = [row[0] for row in cursor.fetchall()]\n",
        "        required_tables = ['players', 'matches', 'teams', 'player_match_stats']\n",
        "        missing_tables = [table for table in required_tables if table not in tables]\n",
        "        if missing_tables:\n",
        "            print(f\"‚ö†Ô∏è Missing tables: {missing_tables}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Found all required tables: {required_tables}\")\n",
        "        conn.close()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Database error: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ PyTorch classes loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìÅ Cell 3: Upload Database File\n",
        "\n",
        "from google.colab import files\n",
        "print(\"üì§ Please upload your valorant_matches.db file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    print(f\"‚úÖ Database uploaded: {db_path}\")\n",
        "    \n",
        "    # Quick verification\n",
        "    file_size = os.path.getsize(db_path) / (1024 * 1024)  # MB\n",
        "    print(f\"üìä File size: {file_size:.2f} MB\")\n",
        "    \n",
        "    if check_database_schema(db_path):\n",
        "        print(\"‚úÖ Database structure verified!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Database structure check failed, but continuing...\")\n",
        "else:\n",
        "    print(\"‚ùå No file uploaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö° Cell 4: Ultra-Fast Classes (Required for Advanced Training)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FastDatabaseDataLoader:\n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def get_connection(self):\n",
        "        return sqlite3.connect(self.db_path)\n",
        "\n",
        "    def calculate_map_features_FAST(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"‚ö° ULTRA-FAST vectorized feature engineering\"\"\"\n",
        "        print(\"‚ö° Starting ULTRA-FAST feature engineering...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Convert and sort\n",
        "        df['match_date'] = pd.to_datetime(df['match_date'])\n",
        "        df = df.sort_values(['consolidated_player_name', 'match_date', 'map_id']).reset_index(drop=True)\n",
        "\n",
        "        # ‚ö° VECTORIZED FEATURES (NO LOOPS!)\n",
        "        print(\"‚ö° Calculating vectorized features...\")\n",
        "        \n",
        "        # Historical averages using rolling with shift (NO DATA LEAKAGE!)\n",
        "        df['hist_avg_kills'] = (\n",
        "            df.groupby('consolidated_player_name')['kills']\n",
        "            .rolling(10, min_periods=1).mean()\n",
        "            .shift(1).reset_index(level=0, drop=True)\n",
        "        ).fillna(15.0)\n",
        "        \n",
        "        df['hist_avg_kdr'] = (\n",
        "            df.groupby('consolidated_player_name')['kdr']\n",
        "            .rolling(10, min_periods=1).mean()\n",
        "            .shift(1).reset_index(level=0, drop=True)\n",
        "        ).fillna(1.0)\n",
        "\n",
        "        # Recent form\n",
        "        df['recent_kills_5'] = (\n",
        "            df.groupby('consolidated_player_name')['kills']\n",
        "            .rolling(5, min_periods=1).mean()\n",
        "            .shift(1).reset_index(level=0, drop=True)\n",
        "        ).fillna(df['hist_avg_kills'])\n",
        "\n",
        "        # Other features\n",
        "        df['days_since_last'] = df.groupby('consolidated_player_name')['match_date'].diff().dt.days.fillna(7.0)\n",
        "\n",
        "        # Series importance\n",
        "        series_importance = {'bo1': 1, 'bo3': 2, 'bo5': 3}\n",
        "        df['series_importance'] = df['series_type'].map(series_importance).fillna(1)\n",
        "        \n",
        "        df = df.fillna(0)\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"üéâ ULTRA-FAST feature engineering completed in {elapsed:.1f} seconds!\")\n",
        "        return df\n",
        "\n",
        "class PlayerConsolidator:\n",
        "    \"\"\"Consolidates duplicate players and manages player identity\"\"\"\n",
        "    \n",
        "    def __init__(self, similarity_threshold=0.85):\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.player_mapping = {}  # original_name -> consolidated_name\n",
        "        self.consolidated_players = {}  # consolidated_name -> list of original names\n",
        "        \n",
        "    def normalize_name(self, name):\n",
        "        \"\"\"Normalize player name for comparison\"\"\"\n",
        "        if pd.isna(name) or name is None:\n",
        "            return \"unknown_player\"\n",
        "        \n",
        "        # Convert to lowercase and remove common suffixes/prefixes\n",
        "        normalized = str(name).lower().strip()\n",
        "        \n",
        "        # Remove team tags and common suffixes\n",
        "        removals = ['_sen', '_c9', '_100t', '_nv', '_tsm', '_lg', '_faze', \n",
        "                   ' (sentinels)', ' (cloud9)', ' (100 thieves)', ' (envy)',\n",
        "                   ' (team solomid)', ' (luminosity)', ' (faze)', '_v1',\n",
        "                   ' (version1)', ' (g2)', '_g2', ' (acend)', '_ace']\n",
        "        \n",
        "        for removal in removals:\n",
        "            normalized = normalized.replace(removal, '')\n",
        "        \n",
        "        # Remove special characters but keep spaces\n",
        "        import re\n",
        "        normalized = re.sub(r'[^\\w\\s]', '', normalized)\n",
        "        normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
        "        \n",
        "        return normalized\n",
        "    \n",
        "    def find_similar_players(self, player_names):\n",
        "        \"\"\"Find groups of similar player names that should be consolidated\"\"\"\n",
        "        print(f\"üîç Analyzing {len(set(player_names))} unique player names for duplicates...\")\n",
        "        \n",
        "        unique_names = list(set(player_names))\n",
        "        normalized_names = {name: self.normalize_name(name) for name in unique_names}\n",
        "        \n",
        "        # Group by exact normalized match first\n",
        "        exact_groups = defaultdict(list)\n",
        "        for original, normalized in normalized_names.items():\n",
        "            exact_groups[normalized].append(original)\n",
        "        \n",
        "        # Find fuzzy matches for remaining singles\n",
        "        consolidated_groups = []\n",
        "        processed = set()\n",
        "        \n",
        "        for normalized, originals in exact_groups.items():\n",
        "            if len(originals) > 1:\n",
        "                # Multiple names normalize to same thing - definitely duplicates\n",
        "                consolidated_groups.append(originals)\n",
        "                processed.update(originals)\n",
        "        \n",
        "        return consolidated_groups\n",
        "    \n",
        "    def consolidate_players(self, df):\n",
        "        \"\"\"Consolidate duplicate players in the dataframe\"\"\"\n",
        "        print(\"\\\\nüîÑ CONSOLIDATING DUPLICATE PLAYERS...\")\n",
        "        \n",
        "        original_players = df['player_name'].nunique()\n",
        "        \n",
        "        # Find duplicate groups\n",
        "        duplicate_groups = self.find_similar_players(df['player_name'].unique())\n",
        "        \n",
        "        # Create mapping from original to consolidated name\n",
        "        for group in duplicate_groups:\n",
        "            # Use the most common name as the consolidated name\n",
        "            name_counts = df[df['player_name'].isin(group)]['player_name'].value_counts()\n",
        "            consolidated_name = name_counts.index[0]  # Most frequent name\n",
        "            \n",
        "            for original_name in group:\n",
        "                self.player_mapping[original_name] = consolidated_name\n",
        "            \n",
        "            self.consolidated_players[consolidated_name] = group\n",
        "        \n",
        "        # Apply consolidation\n",
        "        df['consolidated_player_name'] = df['player_name'].map(\n",
        "            lambda x: self.player_mapping.get(x, x)\n",
        "        )\n",
        "        \n",
        "        # Report consolidation results\n",
        "        final_players = df['consolidated_player_name'].nunique()\n",
        "        duplicates_merged = original_players - final_players\n",
        "        \n",
        "        print(f\"‚úÖ CONSOLIDATION COMPLETE:\")\n",
        "        print(f\"   Original players: {original_players}\")\n",
        "        print(f\"   Final players: {final_players}\")\n",
        "        print(f\"   Duplicates merged: {duplicates_merged}\")\n",
        "        \n",
        "        return df\n",
        "\n",
        "print(\"‚úÖ Ultra-fast classes loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Cell 5: ADVANCED FEATURE TRAINING (THE MAIN ONE!)\n",
        "\n",
        "print(\"üéØ ENHANCED FEATURE ENGINEERING FOR SUB-3 MAE\")\n",
        "print(\"Now with Head-to-Head matchups, Recent Form Momentum, and Map-Opponent Analysis!\")\n",
        "print(\"\")\n",
        "\n",
        "class AdvancedFeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering for esports prediction\"\"\"\n",
        "    \n",
        "    def calculate_matchup_features(self, df):\n",
        "        \"\"\"Calculate matchup evenness and game flow features\"\"\"\n",
        "        print(\"üîç CALCULATING MATCHUP DYNAMICS...\")\n",
        "        \n",
        "        # 1. ROUND COUNT ANALYSIS (More rounds = more kills)\n",
        "        match_round_stats = df.groupby('match_id').agg({\n",
        "            'kills': ['sum', 'count'],\n",
        "            'series_type': 'first'\n",
        "        }).reset_index()\n",
        "        \n",
        "        match_round_stats.columns = ['match_id', 'total_kills', 'total_records', 'series_type']\n",
        "        match_round_stats['estimated_rounds_per_map'] = match_round_stats['total_kills'] / match_round_stats['total_records']\n",
        "        \n",
        "        # Categorize game length\n",
        "        def categorize_game_length(rounds_per_map):\n",
        "            if rounds_per_map >= 16:\n",
        "                return 'long_game'\n",
        "            elif rounds_per_map >= 12:\n",
        "                return 'medium_game'\n",
        "            else:\n",
        "                return 'short_game'\n",
        "        \n",
        "        match_round_stats['game_length_category'] = match_round_stats['estimated_rounds_per_map'].apply(categorize_game_length)\n",
        "        \n",
        "        # 2. MATCHUP EVENNESS (Close games vs stomps)\n",
        "        match_variance = df.groupby('match_id')['kills'].agg(['std', 'mean']).reset_index()\n",
        "        match_variance['kill_cv'] = match_variance['std'] / match_variance['mean']\n",
        "        \n",
        "        def categorize_evenness(cv):\n",
        "            if pd.isna(cv):\n",
        "                return 'unknown'\n",
        "            elif cv <= 0.4:\n",
        "                return 'even_matchup'\n",
        "            elif cv <= 0.7:\n",
        "                return 'somewhat_even'\n",
        "            else:\n",
        "                return 'uneven_stomp'\n",
        "        \n",
        "        match_variance['matchup_evenness'] = match_variance['kill_cv'].apply(categorize_evenness)\n",
        "        \n",
        "        # 3. SERIES PRESSURE\n",
        "        series_pressure = {'bo1': 3, 'bo3': 2, 'bo5': 1}\n",
        "        \n",
        "        # Merge features back\n",
        "        df = df.merge(match_round_stats[['match_id', 'estimated_rounds_per_map', 'game_length_category']], \n",
        "                     on='match_id', how='left')\n",
        "        df = df.merge(match_variance[['match_id', 'kill_cv', 'matchup_evenness']], \n",
        "                     on='match_id', how='left')\n",
        "        \n",
        "        df['series_pressure'] = df['series_type'].map(series_pressure).fillna(2)\n",
        "        \n",
        "        print(f\"   ‚úÖ Added matchup features\")\n",
        "        return df\n",
        "    \n",
        "    def calculate_player_role_features(self, df):\n",
        "        \"\"\"Identify player roles and calculate role-specific features\"\"\"\n",
        "        print(\"üé≠ CALCULATING PLAYER ROLES...\")\n",
        "        \n",
        "        # Calculate player performance percentiles\n",
        "        player_stats = df.groupby('consolidated_player_name').agg({\n",
        "            'kills': ['mean', 'std', 'count'],\n",
        "            'acs': 'mean',\n",
        "            'fk': 'mean',\n",
        "            'kdr': 'mean'\n",
        "        }).reset_index()\n",
        "        \n",
        "        player_stats.columns = ['player_name', 'avg_kills', 'kills_std', 'total_maps', 'avg_acs', 'avg_fk', 'avg_kdr']\n",
        "        \n",
        "        # Only consider players with enough data\n",
        "        experienced_players = player_stats[player_stats['total_maps'] >= 10].copy()\n",
        "        \n",
        "        if len(experienced_players) > 0:\n",
        "            # Calculate percentiles for classification\n",
        "            experienced_players['kills_percentile'] = experienced_players['avg_kills'].rank(pct=True)\n",
        "            experienced_players['acs_percentile'] = experienced_players['avg_acs'].rank(pct=True)\n",
        "            experienced_players['fk_percentile'] = experienced_players['avg_fk'].rank(pct=True)\n",
        "            \n",
        "            # Classify player roles\n",
        "            def classify_player_role(row):\n",
        "                kills_pct = row['kills_percentile']\n",
        "                acs_pct = row['acs_percentile']\n",
        "                fk_pct = row['fk_percentile']\n",
        "                \n",
        "                # Star fragger: Top 20% in kills AND (top 30% in ACS OR FK)\n",
        "                if kills_pct >= 0.8 and (acs_pct >= 0.7 or fk_pct >= 0.7):\n",
        "                    return 'star_fragger'\n",
        "                elif kills_pct >= 0.6:\n",
        "                    return 'secondary_fragger'  \n",
        "                elif kills_pct <= 0.4:\n",
        "                    return 'support_player'\n",
        "                else:\n",
        "                    return 'balanced_player'\n",
        "            \n",
        "            experienced_players['player_role'] = experienced_players.apply(classify_player_role, axis=1)\n",
        "            \n",
        "            # Consistency analysis\n",
        "            experienced_players['consistency_score'] = 1 / (1 + experienced_players['kills_std'])\n",
        "            experienced_players['consistency_tier'] = pd.qcut(experienced_players['consistency_score'], \n",
        "                                                            q=3, labels=['volatile', 'moderate', 'consistent'])\n",
        "            \n",
        "            # Create mappings\n",
        "            role_mapping = dict(zip(experienced_players['player_name'], experienced_players['player_role']))\n",
        "            consistency_mapping = dict(zip(experienced_players['player_name'], experienced_players['consistency_tier']))\n",
        "            \n",
        "        else:\n",
        "            role_mapping = {}\n",
        "            consistency_mapping = {}\n",
        "        \n",
        "        # Apply to main dataframe\n",
        "        df['player_role'] = df['consolidated_player_name'].map(role_mapping).fillna('unknown')\n",
        "        df['consistency_tier'] = df['consolidated_player_name'].map(consistency_mapping).fillna('moderate')\n",
        "        \n",
        "        # Role-specific expectations\n",
        "        role_expectations = {\n",
        "            'star_fragger': 1.3,\n",
        "            'secondary_fragger': 1.1,\n",
        "            'balanced_player': 1.0,\n",
        "            'support_player': 0.8,\n",
        "            'unknown': 1.0\n",
        "        }\n",
        "        \n",
        "        df['role_kill_multiplier'] = df['player_role'].map(role_expectations)\n",
        "        df['facing_star_fraggers'] = 0.5  # Placeholder\n",
        "        \n",
        "        print(f\"   ‚úÖ Identified roles: {df['player_role'].value_counts().to_dict()}\")\n",
        "        return df\n",
        "    \n",
        "    def calculate_meta_features(self, df):\n",
        "        \"\"\"Calculate advanced meta-game features\"\"\"\n",
        "        print(\"üß† CALCULATING META-GAME FEATURES...\")\n",
        "        \n",
        "        # Team synergy\n",
        "        team_performance = df.groupby(['team_name', 'match_id'])['kills'].sum().reset_index()\n",
        "        team_avg_kills = team_performance.groupby('team_name')['kills'].mean().to_dict()\n",
        "        df['team_avg_kills'] = df['team_name'].map(team_avg_kills).fillna(df['kills'].mean())\n",
        "        \n",
        "        # Map specialization\n",
        "        player_map_performance = df.groupby(['consolidated_player_name', 'map_name'])['kills'].mean().reset_index()\n",
        "        overall_player_performance = df.groupby('consolidated_player_name')['kills'].mean().reset_index()\n",
        "        overall_player_performance.columns = ['consolidated_player_name', 'overall_avg_kills']\n",
        "        \n",
        "        map_specialization = player_map_performance.merge(overall_player_performance, on='consolidated_player_name')\n",
        "        map_specialization['map_specialization_factor'] = map_specialization['kills'] / map_specialization['overall_avg_kills']\n",
        "        \n",
        "        map_spec_lookup = {}\n",
        "        for _, row in map_specialization.iterrows():\n",
        "            key = (row['consolidated_player_name'], row['map_name'])\n",
        "            map_spec_lookup[key] = row['map_specialization_factor']\n",
        "        \n",
        "        df['map_specialization'] = df.apply(\n",
        "            lambda row: map_spec_lookup.get((row['consolidated_player_name'], row['map_name']), 1.0), \n",
        "            axis=1\n",
        "        )\n",
        "        \n",
        "        # Form trend (simplified)\n",
        "        df['form_trend'] = 0  # Placeholder for now\n",
        "        \n",
        "        print(\"   ‚úÖ Added meta features\")\n",
        "        return df\n",
        "    \n",
        "    def calculate_head_to_head_features(self, df):\n",
        "        \"\"\"Calculate head-to-head matchup features\"\"\"\n",
        "        print(\"ü•ä CALCULATING HEAD-TO-HEAD MATCHUPS...\")\n",
        "        \n",
        "        # Sort by date to ensure proper chronological order\n",
        "        df = df.sort_values(['match_date', 'match_id', 'map_id']).reset_index(drop=True)\n",
        "        \n",
        "        # Get opponent teams for each match\n",
        "        match_teams = df.groupby('match_id')['team_name'].unique().reset_index()\n",
        "        opponent_mapping = {}\n",
        "        \n",
        "        for _, row in match_teams.iterrows():\n",
        "            match_id = row['match_id']\n",
        "            teams = row['team_name']\n",
        "            if len(teams) >= 2:\n",
        "                team1, team2 = teams[0], teams[1]\n",
        "                opponent_mapping[f\"{match_id}_{team1}\"] = team2\n",
        "                opponent_mapping[f\"{match_id}_{team2}\"] = team1\n",
        "        \n",
        "        # Add opponent team to each record\n",
        "        df['opponent_team'] = df.apply(\n",
        "            lambda row: opponent_mapping.get(f\"{row['match_id']}_{row['team_name']}\", 'unknown'),\n",
        "            axis=1\n",
        "        )\n",
        "        \n",
        "        # Calculate head-to-head performance (avoiding data leakage)\n",
        "        h2h_features = []\n",
        "        \n",
        "        for i, row in df.iterrows():\n",
        "            if i % 50000 == 0:\n",
        "                print(f\"   Processing head-to-head for record {i:,}/{len(df):,}\")\n",
        "                \n",
        "            player = row['consolidated_player_name']\n",
        "            opponent_team = row['opponent_team']\n",
        "            current_match_date = row['match_date']\n",
        "            current_map = row['map_name']\n",
        "            \n",
        "            # Get historical performance against this opponent team (before current match)\n",
        "            historical_vs_opponent = df[\n",
        "                (df['consolidated_player_name'] == player) &\n",
        "                (df['opponent_team'] == opponent_team) &\n",
        "                (df['match_date'] < current_match_date)\n",
        "            ]\n",
        "            \n",
        "            if len(historical_vs_opponent) >= 3:\n",
        "                h2h_avg_kills = historical_vs_opponent['kills'].mean()\n",
        "                h2h_kill_trend = historical_vs_opponent.tail(5)['kills'].mean() - historical_vs_opponent.head(5)['kills'].mean()\n",
        "                h2h_map_performance = historical_vs_opponent[\n",
        "                    historical_vs_opponent['map_name'] == current_map\n",
        "                ]['kills'].mean() if len(historical_vs_opponent[historical_vs_opponent['map_name'] == current_map]) > 0 else h2h_avg_kills\n",
        "            elif len(historical_vs_opponent) >= 1:\n",
        "                h2h_avg_kills = historical_vs_opponent['kills'].mean()\n",
        "                h2h_kill_trend = 0\n",
        "                h2h_map_performance = h2h_avg_kills\n",
        "            else:\n",
        "                h2h_avg_kills = row['hist_avg_kills']  # Fall back to overall average\n",
        "                h2h_kill_trend = 0\n",
        "                h2h_map_performance = row['hist_avg_kills']\n",
        "            \n",
        "            h2h_features.append({\n",
        "                'h2h_avg_kills': h2h_avg_kills,\n",
        "                'h2h_kill_trend': h2h_kill_trend,\n",
        "                'h2h_map_performance': h2h_map_performance,\n",
        "                'h2h_matches_played': len(historical_vs_opponent)\n",
        "            })\n",
        "        \n",
        "        # Add features to dataframe\n",
        "        h2h_df = pd.DataFrame(h2h_features)\n",
        "        for col in h2h_df.columns:\n",
        "            df[col] = h2h_df[col]\n",
        "        \n",
        "        print(\"   ‚úÖ Added head-to-head features\")\n",
        "        return df\n",
        "    \n",
        "    def calculate_momentum_features(self, df):\n",
        "        \"\"\"Calculate recent form and momentum features\"\"\"\n",
        "        print(\"üìà CALCULATING RECENT FORM MOMENTUM...\")\n",
        "        \n",
        "        # Sort by player and date\n",
        "        df = df.sort_values(['consolidated_player_name', 'match_date', 'map_id']).reset_index(drop=True)\n",
        "        \n",
        "        # Calculate momentum features using vectorized operations\n",
        "        momentum_features = []\n",
        "        \n",
        "        # Group by player for efficient calculation\n",
        "        for player, player_df in df.groupby('consolidated_player_name'):\n",
        "            player_df = player_df.reset_index(drop=True)\n",
        "            \n",
        "            # Calculate rolling statistics (with shift to avoid data leakage)\n",
        "            recent_3_kills = player_df['kills'].rolling(3, min_periods=1).mean().shift(1).fillna(player_df['kills'].iloc[0] if len(player_df) > 0 else 15.0)\n",
        "            recent_5_kills = player_df['kills'].rolling(5, min_periods=1).mean().shift(1).fillna(player_df['kills'].iloc[0] if len(player_df) > 0 else 15.0)\n",
        "            \n",
        "            # Momentum trend (recent vs earlier performance)\n",
        "            momentum_trend = recent_3_kills - recent_5_kills\n",
        "            \n",
        "            # Kill consistency (rolling standard deviation)\n",
        "            kill_consistency = 1 / (1 + player_df['kills'].rolling(5, min_periods=1).std().shift(1).fillna(1.0))\n",
        "            \n",
        "            # Hot/cold streak detection\n",
        "            hot_streak = (recent_3_kills > player_df['kills'].rolling(10, min_periods=1).mean().shift(1)).astype(int)\n",
        "            \n",
        "            # Performance vs expectation\n",
        "            expected_kills = player_df['kills'].expanding().mean().shift(1).fillna(15.0)\n",
        "            recent_vs_expected = recent_3_kills - expected_kills\n",
        "            \n",
        "            # Store features for this player\n",
        "            for i in range(len(player_df)):\n",
        "                momentum_features.append({\n",
        "                    'recent_momentum_3': recent_3_kills.iloc[i],\n",
        "                    'momentum_trend': momentum_trend.iloc[i],\n",
        "                    'kill_consistency': kill_consistency.iloc[i],\n",
        "                    'hot_streak': hot_streak.iloc[i],\n",
        "                    'recent_vs_expected': recent_vs_expected.iloc[i]\n",
        "                })\n",
        "        \n",
        "        # Add features to dataframe\n",
        "        momentum_df = pd.DataFrame(momentum_features)\n",
        "        for col in momentum_df.columns:\n",
        "            df[col] = momentum_df[col]\n",
        "        \n",
        "        print(\"   ‚úÖ Added momentum features\")\n",
        "        return df\n",
        "    \n",
        "    def calculate_map_opponent_features(self, df):\n",
        "        \"\"\"Calculate map-specific opponent analysis\"\"\"\n",
        "        print(\"üó∫Ô∏è CALCULATING MAP-SPECIFIC OPPONENT ANALYSIS...\")\n",
        "        \n",
        "        map_opponent_features = []\n",
        "        \n",
        "        for i, row in df.iterrows():\n",
        "            if i % 50000 == 0:\n",
        "                print(f\"   Processing map-opponent analysis for record {i:,}/{len(df):,}\")\n",
        "                \n",
        "            player = row['consolidated_player_name']\n",
        "            opponent_team = row['opponent_team']\n",
        "            current_map = row['map_name']\n",
        "            current_match_date = row['match_date']\n",
        "            \n",
        "            # Historical performance on this specific map against this opponent\n",
        "            map_opponent_history = df[\n",
        "                (df['consolidated_player_name'] == player) &\n",
        "                (df['opponent_team'] == opponent_team) &\n",
        "                (df['map_name'] == current_map) &\n",
        "                (df['match_date'] < current_match_date)\n",
        "            ]\n",
        "            \n",
        "            # Map performance against all opponents (for comparison)\n",
        "            map_overall_history = df[\n",
        "                (df['consolidated_player_name'] == player) &\n",
        "                (df['map_name'] == current_map) &\n",
        "                (df['match_date'] < current_match_date)\n",
        "            ]\n",
        "            \n",
        "            # Opponent difficulty on this map (how they generally perform)\n",
        "            opponent_map_strength = df[\n",
        "                (df['team_name'] == opponent_team) &\n",
        "                (df['map_name'] == current_map) &\n",
        "                (df['match_date'] < current_match_date)\n",
        "            ]['kills'].mean() if len(df[\n",
        "                (df['team_name'] == opponent_team) &\n",
        "                (df['map_name'] == current_map) &\n",
        "                (df['match_date'] < current_match_date)\n",
        "            ]) > 0 else df['kills'].mean()\n",
        "            \n",
        "            if len(map_opponent_history) >= 2:\n",
        "                map_opponent_avg = map_opponent_history['kills'].mean()\n",
        "                map_opponent_consistency = 1 / (1 + map_opponent_history['kills'].std())\n",
        "            elif len(map_overall_history) >= 3:\n",
        "                map_opponent_avg = map_overall_history['kills'].mean()\n",
        "                map_opponent_consistency = 1 / (1 + map_overall_history['kills'].std())\n",
        "            else:\n",
        "                map_opponent_avg = row['hist_avg_kills']\n",
        "                map_opponent_consistency = 0.5\n",
        "            \n",
        "            # Map difficulty factor (how challenging this map generally is)\n",
        "            overall_map_avg = df[df['map_name'] == current_map]['kills'].mean()\n",
        "            map_difficulty = overall_map_avg / df['kills'].mean() if df['kills'].mean() > 0 else 1.0\n",
        "            \n",
        "            map_opponent_features.append({\n",
        "                'map_opponent_avg': map_opponent_avg,\n",
        "                'map_opponent_consistency': map_opponent_consistency,\n",
        "                'opponent_map_strength': opponent_map_strength,\n",
        "                'map_difficulty_factor': map_difficulty,\n",
        "                'map_opponent_experience': len(map_opponent_history)\n",
        "            })\n",
        "        \n",
        "        # Add features to dataframe\n",
        "        map_opp_df = pd.DataFrame(map_opponent_features)\n",
        "        for col in map_opp_df.columns:\n",
        "            df[col] = map_opp_df[col]\n",
        "        \n",
        "        print(\"   ‚úÖ Added map-opponent features\")\n",
        "        return df\n",
        "\n",
        "class AdvancedTrainer:\n",
        "    \"\"\"Advanced trainer with sophisticated feature engineering\"\"\"\n",
        "    \n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self.scaler = StandardScaler()\n",
        "        self.consolidator = PlayerConsolidator()\n",
        "        self.feature_engineer = AdvancedFeatureEngineer()\n",
        "        \n",
        "    def train_advanced_model(self):\n",
        "        \"\"\"Train model with all advanced features\"\"\"\n",
        "        print(\"üéØ STARTING ADVANCED TRAINING WITH ALL SOPHISTICATED FEATURES\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # Load ALL data (bypass date filter)\n",
        "        loader = FastDatabaseDataLoader(db_path=self.db_path)\n",
        "        \n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            p.name as player_name, t.name as team_name, pms.team_id as team_id,\n",
        "            m.match_date, m.series_type, tour.name as tournament_name,\n",
        "            mp.map_name, pms.kills, pms.deaths, pms.assists, pms.acs, pms.adr,\n",
        "            pms.fk, pms.hs_percentage, pms.kdr, m.match_id, pms.map_id\n",
        "        FROM player_match_stats pms\n",
        "        JOIN players p ON pms.player_id = p.id\n",
        "        JOIN teams t ON pms.team_id = t.id\n",
        "        JOIN matches m ON pms.match_id = m.id\n",
        "        JOIN maps mp ON pms.map_id = mp.id\n",
        "        JOIN tournaments tour ON m.tournament_id = tour.id\n",
        "        ORDER BY p.name, m.match_date, pms.map_id\n",
        "        \"\"\"\n",
        "        \n",
        "        with loader.get_connection() as conn:\n",
        "            df = pd.read_sql_query(query, conn)\n",
        "        \n",
        "        print(f\"üìä Loaded {len(df)} records from {df['player_name'].nunique()} players\")\n",
        "        \n",
        "        # Player consolidation and confidence weighting\n",
        "        df = self.consolidator.consolidate_players(df)\n",
        "        \n",
        "        # Calculate confidence weights\n",
        "        player_map_counts = df['consolidated_player_name'].value_counts()\n",
        "        weights = {}\n",
        "        for player, count in player_map_counts.items():\n",
        "            if count >= 20:\n",
        "                weight = 1.0\n",
        "            elif count >= 10:\n",
        "                weight = 0.8\n",
        "            elif count >= 5:\n",
        "                weight = 0.6\n",
        "            elif count >= 3:\n",
        "                weight = 0.4\n",
        "            else:\n",
        "                weight = 0.2\n",
        "            weights[player] = weight\n",
        "        \n",
        "        df['player_weight'] = df['consolidated_player_name'].map(weights)\n",
        "        \n",
        "        # ‚≠ê ADVANCED FEATURE ENGINEERING\n",
        "        df = self.feature_engineer.calculate_matchup_features(df)\n",
        "        df = self.feature_engineer.calculate_player_role_features(df) \n",
        "        df = self.feature_engineer.calculate_meta_features(df)\n",
        "        \n",
        "        # üöÄ NEW ELITE FEATURES\n",
        "        df = self.feature_engineer.calculate_head_to_head_features(df)\n",
        "        df = self.feature_engineer.calculate_momentum_features(df)\n",
        "        df = self.feature_engineer.calculate_map_opponent_features(df)\n",
        "        \n",
        "        # Basic feature engineering\n",
        "        print(\"\\\\n‚ö° CALCULATING BASIC FEATURES...\")\n",
        "        df = loader.calculate_map_features_FAST(df)\n",
        "        \n",
        "        # Prepare advanced feature set\n",
        "        advanced_features = [\n",
        "            # Basic features\n",
        "            'hist_avg_kills', 'hist_avg_kdr', 'recent_kills_5', 'days_since_last', 'series_importance',\n",
        "            # Matchup dynamics  \n",
        "            'estimated_rounds_per_map', 'kill_cv', 'series_pressure',\n",
        "            # Player role features\n",
        "            'role_kill_multiplier', 'facing_star_fraggers',\n",
        "            # Meta features\n",
        "            'team_avg_kills', 'map_specialization', 'form_trend',\n",
        "            # ü•ä Head-to-head features\n",
        "            'h2h_avg_kills', 'h2h_kill_trend', 'h2h_map_performance', 'h2h_matches_played',\n",
        "            # üìà Momentum features  \n",
        "            'recent_momentum_3', 'momentum_trend', 'kill_consistency', 'hot_streak', 'recent_vs_expected',\n",
        "            # üó∫Ô∏è Map-opponent features\n",
        "            'map_opponent_avg', 'map_opponent_consistency', 'opponent_map_strength', 'map_difficulty_factor', 'map_opponent_experience'\n",
        "        ]\n",
        "        \n",
        "        # Encode categorical features\n",
        "        le_game_length = LabelEncoder()\n",
        "        df['game_length_encoded'] = le_game_length.fit_transform(df['game_length_category'].fillna('medium_game'))\n",
        "        advanced_features.append('game_length_encoded')\n",
        "        \n",
        "        le_evenness = LabelEncoder() \n",
        "        df['matchup_evenness_encoded'] = le_evenness.fit_transform(df['matchup_evenness'].fillna('somewhat_even'))\n",
        "        advanced_features.append('matchup_evenness_encoded')\n",
        "        \n",
        "        le_role = LabelEncoder()\n",
        "        df['player_role_encoded'] = le_role.fit_transform(df['player_role'].fillna('balanced_player'))\n",
        "        advanced_features.append('player_role_encoded')\n",
        "        \n",
        "        le_consistency = LabelEncoder()\n",
        "        df['consistency_encoded'] = le_consistency.fit_transform(df['consistency_tier'].fillna('moderate'))\n",
        "        advanced_features.append('consistency_encoded')\n",
        "        \n",
        "        # Filter available features\n",
        "        available_features = [col for col in advanced_features if col in df.columns]\n",
        "        print(f\"\\\\nüéØ Using {len(available_features)} advanced features\")\n",
        "        \n",
        "        # Prepare training data\n",
        "        X = df[available_features].fillna(0).values\n",
        "        y = df['kills'].values\n",
        "        weights_array = df['player_weight'].values\n",
        "        \n",
        "        print(f\"\\\\n‚úÖ Advanced training data prepared:\")\n",
        "        print(f\"   Samples: {len(X):,}\")\n",
        "        print(f\"   Features: {len(available_features)}\")\n",
        "        print(f\"   Consolidated players: {df['consolidated_player_name'].nunique():,}\")\n",
        "        \n",
        "        # Training\n",
        "        X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
        "            X, y, weights_array, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "        y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
        "        w_train_tensor = torch.FloatTensor(w_train).to(device)\n",
        "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "        y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
        "        \n",
        "        # Advanced model architecture\n",
        "        input_size = X_train_tensor.shape[1]\n",
        "        model = KillPredictionNN(input_size, hidden_sizes=[256, 128, 64, 32]).to(device)\n",
        "        \n",
        "        # Weighted loss function\n",
        "        def weighted_mse_loss(predictions, targets, weights):\n",
        "            squared_errors = (predictions - targets) ** 2\n",
        "            weighted_errors = squared_errors * weights\n",
        "            return torch.mean(weighted_errors)\n",
        "        \n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "        \n",
        "        # Training setup\n",
        "        class WeightedDataset:\n",
        "            def __init__(self, X, y, weights):\n",
        "                self.X = X\n",
        "                self.y = y\n",
        "                self.weights = weights\n",
        "            \n",
        "            def __len__(self):\n",
        "                return len(self.X)\n",
        "            \n",
        "            def __getitem__(self, idx):\n",
        "                return self.X[idx], self.y[idx], self.weights[idx]\n",
        "        \n",
        "        train_dataset = WeightedDataset(X_train_tensor.cpu().numpy(), \n",
        "                                      y_train_tensor.cpu().numpy(), \n",
        "                                      w_train_tensor.cpu().numpy())\n",
        "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "        \n",
        "        # Training loop\n",
        "        print(f\"\\\\nüß† TRAINING ADVANCED MODEL...\")\n",
        "        best_mae = float('inf')\n",
        "        patience = 0\n",
        "        \n",
        "        for epoch in range(100):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            \n",
        "            for batch_X, batch_y, batch_w in train_loader:\n",
        "                batch_X = batch_X.to(device)\n",
        "                batch_y = batch_y.to(device) \n",
        "                batch_w = batch_w.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X).squeeze()\n",
        "                loss = weighted_mse_loss(outputs, batch_y, batch_w)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_test_tensor).squeeze()\n",
        "                    val_mae = mean_absolute_error(y_test_tensor.cpu().numpy(), \n",
        "                                                val_outputs.cpu().numpy())\n",
        "                \n",
        "                print(f\"Epoch {epoch}: Weighted Loss = {train_loss/len(train_loader):.4f}, MAE = {val_mae:.3f}\")\n",
        "                \n",
        "                # Early stopping\n",
        "                if val_mae < best_mae:\n",
        "                    best_mae = val_mae\n",
        "                    patience = 0\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience >= 3:\n",
        "                        print(f\"Early stopping at epoch {epoch}\")\n",
        "                        break\n",
        "        \n",
        "        # Final evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(X_test_tensor).squeeze().cpu().numpy()\n",
        "            y_test_np = y_test_tensor.cpu().numpy()\n",
        "        \n",
        "        mse = mean_squared_error(y_test_np, y_pred)\n",
        "        mae = mean_absolute_error(y_test_np, y_pred)\n",
        "        r2 = r2_score(y_test_np, y_pred)\n",
        "        \n",
        "        # Results analysis\n",
        "        print(f\"\\\\nüéâ ADVANCED TRAINING COMPLETED!\")\n",
        "        print(f\"üéØ MAE: {mae:.3f} kills per map\")\n",
        "        print(f\"üìà R¬≤: {r2:.6f}\")\n",
        "        \n",
        "        # Achievement analysis\n",
        "        if mae <= 3.0:\n",
        "            print(f\"\\\\nüèÜ OUTSTANDING! MAE of {mae:.2f} is approaching your 1-3 target!\")\n",
        "        elif mae <= 5.0:\n",
        "            print(f\"\\\\nüéâ EXCELLENT! MAE of {mae:.2f} is substantial improvement!\")\n",
        "        elif mae <= 6.5:\n",
        "            print(f\"\\\\nüéØ GOOD PROGRESS! MAE of {mae:.2f} shows advanced features help!\")\n",
        "        \n",
        "        # Save advanced model\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        model_data = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'input_size': input_size,\n",
        "            'hidden_sizes': [256, 128, 64, 32],\n",
        "            'scaler': self.scaler,\n",
        "            'feature_columns': available_features,\n",
        "            'performance': {'mse': mse, 'mae': mae, 'r2': r2}\n",
        "        }\n",
        "        \n",
        "        joblib.dump(model_data, 'models/enhanced_elite_model.pkl')\n",
        "        print(f\"\\\\n‚úÖ Enhanced model saved as 'enhanced_elite_model.pkl'!\")\n",
        "        \n",
        "        return model_data\n",
        "\n",
        "# Usage\n",
        "if 'uploaded' in globals() and uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    print(f\"üéØ Starting ADVANCED training with: {db_path}\")\n",
        "    \n",
        "    try:\n",
        "        trainer = AdvancedTrainer(db_path=db_path)\n",
        "        model_data = trainer.train_advanced_model()\n",
        "        \n",
        "        print(\"\\\\nüéâ SUCCESS! Your model now features:\")\n",
        "        print(\"   ‚úÖ Matchup dynamics (even vs uneven games)\")\n",
        "        print(\"   ‚úÖ Player role modeling (star fraggers vs supports - no bias)\")\n",
        "        print(\"   ‚úÖ Game length prediction (more rounds = more kills)\")\n",
        "        print(\"   ‚úÖ Team synergy effects\")\n",
        "        print(\"   ‚úÖ Map specialization\")\n",
        "        print(\"   ‚úÖ ü•ä Head-to-head opponent analysis\")\n",
        "        print(\"   ‚úÖ üìà Recent form momentum & hot streaks\")\n",
        "        print(\"   ‚úÖ üó∫Ô∏è Map-specific opponent performance\")\n",
        "        print(\"   ‚úÖ Professional-grade feature engineering!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"‚ùå Please upload your database file first (run Cell 3)\")\n",
        "\n",
        "print(\"\\\\nüí° ADVANCED FEATURES EXPLAINED:\")\n",
        "print(\"   üéÆ Game Length: Long games = more rounds = higher kill totals\")\n",
        "print(\"   ‚öñÔ∏è Matchup Evenness: Close games vs stomps affect individual performance\")\n",
        "print(\"   üé≠ Player Roles: Star fraggers vs support players (no bias - purely stats-based)\")\n",
        "print(\"   üó∫Ô∏è Map Specialization: Player performance varies by map\")\n",
        "print(\"   ü§ù Team Synergy: How well teams work together affects individual stats\")\n",
        "print(\"   üèÜ Series Pressure: BO1 vs BO3 vs BO5 psychological effects\")\n",
        "print(\"   ü•ä Head-to-Head: How players perform specifically against certain opponents\")\n",
        "print(\"   üìà Momentum: Recent form trends, hot/cold streaks, consistency patterns\")\n",
        "print(\"   üó∫Ô∏èü•ä Map-Opponent: Performance on specific maps against specific teams\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì• Cell 6: Download Trained Model\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    if os.path.exists('models/enhanced_elite_model.pkl'):\n",
        "        print(\"üì¶ Downloading your enhanced elite trained model...\")\n",
        "        files.download('models/enhanced_elite_model.pkl')\n",
        "        print(\"‚úÖ Model downloaded successfully!\")\n",
        "        print(\"\\\\nüéØ You can now use this model to make kill predictions!\")\n",
        "        print(\"\\\\nüìã What you got:\")\n",
        "        print(\"  üß† Enhanced neural network model with elite features\")\n",
        "        print(\"  üìä Feature scaler and encoders\") \n",
        "        print(\"  üìà Performance metrics\")\n",
        "        print(\"  üé≠ Player role modeling (unbiased)\")\n",
        "        print(\"  ‚öñÔ∏è Matchup dynamics analysis\")\n",
        "        print(\"  üó∫Ô∏è Map specialization factors\")\n",
        "        print(\"  ü•ä Head-to-head opponent analysis\")\n",
        "        print(\"  üìà Recent form momentum tracking\")\n",
        "        print(\"  üó∫Ô∏èü•ä Map-specific opponent performance\")\n",
        "    else:\n",
        "        print(\"‚ùå No trained model found. Please run Cell 5 first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Download error: {e}\")\n",
        "    print(\"üí° You can find the model in the files panel on the left.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
