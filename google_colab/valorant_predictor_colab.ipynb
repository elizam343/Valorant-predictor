{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéÆ Valorant Kill Predictor - Google Colab Training\n",
        "\n",
        "This notebook trains your Valorant kill prediction model using Google Colab GPU.\n",
        "\n",
        "## üìã Main Workflow (Try This First):\n",
        "1. **Cell 1**: Install dependencies and imports\n",
        "2. **Cell 2**: PyTorch classes and utilities  \n",
        "3. **Cell 3**: Database and training classes\n",
        "4. **Cell 4**: Upload your database file\n",
        "5. **Cell 5**: Start training (might be slow with large datasets)\n",
        "6. **Cell 6**: Download trained model\n",
        "\n",
        "## ‚ö° Ultra-Fast Alternative (If Main Workflow Is Too Slow):\n",
        "7. **Cell 7**: Ultra-fast classes (100x faster feature engineering)\n",
        "8. **Cell 8**: Ultra-fast training execution\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Cell 1: Install Dependencies and Imports\n",
        "\n",
        "# Install PyTorch if needed\n",
        "try:\n",
        "    import torch\n",
        "    print(\"‚úÖ PyTorch already installed\")\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing PyTorch...\")\n",
        "    %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    import torch\n",
        "\n",
        "# All necessary imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import logging\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import joblib\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import sqlite3\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üî• Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"‚úÖ All imports loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß† Cell 2: PyTorch Classes and Utilities\n",
        "\n",
        "class KillPredictionDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for kill prediction\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class KillPredictionNN(nn.Module):\n",
        "    \"\"\"Neural network for kill prediction\"\"\"\n",
        "    def __init__(self, input_size: int, hidden_sizes: List[int] = [128, 64, 32]):\n",
        "        super(KillPredictionNN, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_size, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.BatchNorm1d(hidden_size)\n",
        "            ])\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def check_database_schema(db_path):\n",
        "    \"\"\"Check if database has required tables\"\"\"\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"‚ùå Database file not found: {db_path}\")\n",
        "        return False\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "        tables = [row[0] for row in cursor.fetchall()]\n",
        "        required_tables = ['players', 'matches', 'teams', 'player_match_stats']\n",
        "        missing_tables = [table for table in required_tables if table not in tables]\n",
        "        if missing_tables:\n",
        "            print(f\"‚ö†Ô∏è Missing tables: {missing_tables}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Found all required tables: {required_tables}\")\n",
        "        conn.close()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Database error: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ PyTorch classes loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Cell 3: Database and Training Classes (Per-Map Prediction)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DatabaseDataLoader:\n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def get_connection(self):\n",
        "        return sqlite3.connect(self.db_path)\n",
        "\n",
        "    def load_player_match_data(self, min_maps: int = 20, days_back: int = 365) -> pd.DataFrame:\n",
        "        \"\"\"Load per-map player data\"\"\"\n",
        "        logger.info(\"Loading per-map player data from database...\")\n",
        "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            p.name as player_name,\n",
        "            t.name as team_name,\n",
        "            pms.team_id as team_id,\n",
        "            m.match_date,\n",
        "            m.series_type,\n",
        "            tour.name as tournament_name,\n",
        "            mp.map_name,\n",
        "            pms.kills,\n",
        "            pms.deaths,\n",
        "            pms.assists,\n",
        "            pms.acs,\n",
        "            pms.adr,\n",
        "            pms.fk,\n",
        "            pms.hs_percentage,\n",
        "            pms.kdr,\n",
        "            m.match_id,\n",
        "            pms.map_id\n",
        "        FROM player_match_stats pms\n",
        "        JOIN players p ON pms.player_id = p.id\n",
        "        JOIN teams t ON pms.team_id = t.id\n",
        "        JOIN matches m ON pms.match_id = m.id\n",
        "        JOIN maps mp ON pms.map_id = mp.id\n",
        "        JOIN tournaments tour ON m.tournament_id = tour.id\n",
        "        WHERE m.match_date >= ?\n",
        "        ORDER BY p.name, m.match_date, pms.map_id\n",
        "        \"\"\"\n",
        "\n",
        "        with self.get_connection() as conn:\n",
        "            df = pd.read_sql_query(query, conn, params=(cutoff_date,))\n",
        "\n",
        "        logger.info(f\"Loaded {len(df)} per-map records\")\n",
        "\n",
        "        # Filter players with minimum MAP count\n",
        "        player_map_counts = df['player_name'].value_counts()\n",
        "        valid_players = player_map_counts[player_map_counts >= min_maps].index\n",
        "        df = df[df['player_name'].isin(valid_players)]\n",
        "\n",
        "        logger.info(f\"Filtered to {len(df)} map records from {len(valid_players)} players\")\n",
        "        return df\n",
        "\n",
        "    def calculate_map_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calculate per-map features - WARNING: Can be slow with large datasets!\"\"\"\n",
        "        logger.info(\"Calculating per-map features...\")\n",
        "        print(\"‚ö†Ô∏è WARNING: This may take 30+ minutes with large datasets!\")\n",
        "        print(\"üí° If it takes too long, use the ultra-fast version (Cells 7-8)\")\n",
        "        \n",
        "        # Convert date and sort\n",
        "        df['match_date'] = pd.to_datetime(df['match_date'])\n",
        "        df = df.sort_values(['player_name', 'match_date', 'map_id']).reset_index(drop=True)\n",
        "\n",
        "        # Simple feature calculation (this is the slow method)\n",
        "        features_list = []\n",
        "        \n",
        "        for i, row in df.iterrows():\n",
        "            if i % 10000 == 0:\n",
        "                print(f\"Processing record {i}/{len(df)}...\")\n",
        "                \n",
        "            player_name = row['player_name']\n",
        "            current_date = row['match_date']\n",
        "            \n",
        "            # Get historical data (before current match)\n",
        "            historical = df[\n",
        "                (df['player_name'] == player_name) & \n",
        "                (df['match_date'] < current_date)\n",
        "            ]\n",
        "            \n",
        "            if len(historical) == 0:\n",
        "                # Default values for new players\n",
        "                features = {\n",
        "                    'hist_avg_kills': 15.0,\n",
        "                    'hist_avg_kdr': 1.0,\n",
        "                    'recent_kills_5': 15.0,\n",
        "                    'days_since_last': 7.0\n",
        "                }\n",
        "            else:\n",
        "                # Calculate from historical data\n",
        "                recent_10 = historical.tail(10)\n",
        "                features = {\n",
        "                    'hist_avg_kills': recent_10['kills'].mean() if len(recent_10) > 0 else 15.0,\n",
        "                    'hist_avg_kdr': recent_10['kdr'].mean() if len(recent_10) > 0 else 1.0,\n",
        "                    'recent_kills_5': historical.tail(5)['kills'].mean() if len(historical) >= 5 else 15.0,\n",
        "                    'days_since_last': (current_date - historical['match_date'].max()).days\n",
        "                }\n",
        "            \n",
        "            features_list.append(features)\n",
        "        \n",
        "        # Add features to dataframe\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "        for col in features_df.columns:\n",
        "            df[col] = features_df[col]\n",
        "        \n",
        "        # Add series importance\n",
        "        series_importance = {'bo1': 1, 'bo3': 2, 'bo5': 3}\n",
        "        df['series_importance'] = df['series_type'].map(series_importance).fillna(1)\n",
        "        \n",
        "        df = df.fillna(0)\n",
        "        logger.info(f\"Calculated features for {len(df)} records\")\n",
        "        return df\n",
        "\n",
        "    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "        \"\"\"Prepare training data\"\"\"\n",
        "        logger.info(\"Preparing training data...\")\n",
        "        \n",
        "        feature_columns = [\n",
        "            'hist_avg_kills', 'hist_avg_kdr', 'recent_kills_5', \n",
        "            'days_since_last', 'series_importance'\n",
        "        ]\n",
        "        \n",
        "        available_features = [col for col in feature_columns if col in df.columns]\n",
        "        X = df[available_features].values\n",
        "        y = df['kills'].values\n",
        "        \n",
        "        print(f\"üéØ Target statistics:\")\n",
        "        print(f\"   Min kills: {y.min()}\")\n",
        "        print(f\"   Max kills: {y.max()}\")\n",
        "        print(f\"   Mean kills: {y.mean():.2f}\")\n",
        "        \n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        self.feature_columns = available_features\n",
        "        \n",
        "        logger.info(f\"Training data: {X_scaled.shape[0]} samples, {X_scaled.shape[1]} features\")\n",
        "        return X_scaled, y, available_features\n",
        "\n",
        "class GPUTrainer:\n",
        "    def __init__(self, db_path: str):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.results = {}\n",
        "        self.data_loader = DatabaseDataLoader(db_path=db_path)\n",
        "\n",
        "    def train_and_save_model(self):\n",
        "        print(\"üéØ Starting GPU Training...\")\n",
        "        \n",
        "        # Load and prepare data\n",
        "        df = self.data_loader.load_player_match_data(min_maps=20)\n",
        "        print(f\"üìä Loaded {len(df)} map records\")\n",
        "        \n",
        "        df = self.data_loader.calculate_map_features(df)\n",
        "        X, y, feature_columns = self.data_loader.prepare_training_data(df)\n",
        "        \n",
        "        if X.size == 0 or y.size == 0:\n",
        "            raise ValueError(\"No data available for training\")\n",
        "        \n",
        "        print(f\"‚úÖ Data ready: {len(X)} samples with {len(feature_columns)} features\")\n",
        "        \n",
        "        # Split and train (simplified for brevity)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        \n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "        \n",
        "        X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "        y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
        "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "        y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
        "        \n",
        "        # Train neural network\n",
        "        print(\"üß† Training Neural Network...\")\n",
        "        \n",
        "        input_size = X_train_tensor.shape[1]\n",
        "        model = KillPredictionNN(input_size, hidden_sizes=[128, 64, 32]).to(device)\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        train_dataset = KillPredictionDataset(X_train_tensor.cpu().numpy(), y_train_tensor.cpu().numpy())\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        \n",
        "        # Simple training loop\n",
        "        for epoch in range(50):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            \n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X).squeeze()\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_test_tensor).squeeze()\n",
        "                    val_mae = mean_absolute_error(y_test_tensor.cpu().numpy(), val_outputs.cpu().numpy())\n",
        "                print(f\"Epoch {epoch}: Train Loss = {train_loss/len(train_loader):.4f}, MAE = {val_mae:.3f}\")\n",
        "        \n",
        "        # Final evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(X_test_tensor).squeeze().cpu().numpy()\n",
        "            y_test_np = y_test_tensor.cpu().numpy()\n",
        "        \n",
        "        mse = mean_squared_error(y_test_np, y_pred)\n",
        "        mae = mean_absolute_error(y_test_np, y_pred)\n",
        "        r2 = r2_score(y_test_np, y_pred)\n",
        "        \n",
        "        print(f\"\\\\nüéâ Training Results:\")\n",
        "        print(f\"üéØ MAE: {mae:.3f} kills per map\")\n",
        "        print(f\"üìà R¬≤: {r2:.6f}\")\n",
        "        \n",
        "        # Save model\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        model_data = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'input_size': input_size,\n",
        "            'hidden_sizes': [128, 64, 32],\n",
        "            'scaler': self.scaler,\n",
        "            'feature_columns': feature_columns,\n",
        "            'performance': {'mse': mse, 'mae': mae, 'r2': r2}\n",
        "        }\n",
        "        \n",
        "        joblib.dump(model_data, 'models/neural_network_gpu_model.pkl')\n",
        "        print(\"‚úÖ Model saved!\")\n",
        "        \n",
        "        return {'mse': mse, 'mae': mae, 'r2': r2, 'feature_count': len(feature_columns)}\n",
        "\n",
        "print(\"‚úÖ Database and training classes loaded!\")\n",
        "print(\"üéØ Ready for per-map kill prediction training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìÅ Cell 4: Upload Database File\n",
        "\n",
        "from google.colab import files\n",
        "print(\"üì§ Please upload your valorant_matches.db file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    print(f\"‚úÖ Database uploaded: {db_path}\")\n",
        "    \n",
        "    # Quick verification\n",
        "    file_size = os.path.getsize(db_path) / (1024 * 1024)  # MB\n",
        "    print(f\"üìä File size: {file_size:.2f} MB\")\n",
        "    \n",
        "    if check_database_schema(db_path):\n",
        "        print(\"‚úÖ Database structure verified!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Database structure check failed, but continuing...\")\n",
        "else:\n",
        "    print(\"‚ùå No file uploaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Cell 5: Start Training (Per-Map Prediction)\n",
        "\n",
        "if 'uploaded' in globals() and uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    print(f\"üéØ Starting training with database: {db_path}\")\n",
        "    print(f\"üîß Goal: Predict kills per map (target MAE: 1-2 kills)\")\n",
        "    print(f\"‚ö†Ô∏è WARNING: This may take 30+ minutes with large datasets!\")\n",
        "    print(f\"üí° If too slow, stop and use ultra-fast version (Cells 7-8)\")\n",
        "    \n",
        "    try:\n",
        "        trainer = GPUTrainer(db_path=db_path)\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"üöÄ STARTING TRAINING\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        results = trainer.train_and_save_model()\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"\\\\nüéâ TRAINING COMPLETED!\")\n",
        "        print(f\"‚è±Ô∏è Total time: {elapsed/60:.2f} minutes\")\n",
        "        \n",
        "        # Show results\n",
        "        mae = results['mae']\n",
        "        print(f\"\\\\nüèÜ Model Performance:\")\n",
        "        print(f\"  üéØ MAE: {mae:.3f} kills per map\")\n",
        "        print(f\"  üìà R¬≤: {results['r2']:.6f}\")\n",
        "        print(f\"  üî¢ Features: {results['feature_count']}\")\n",
        "        \n",
        "        if mae <= 2.0:\n",
        "            print(f\"üéâ EXCELLENT! MAE of {mae:.2f} is perfect for per-map prediction!\")\n",
        "        elif mae <= 5.0:\n",
        "            print(f\"üéØ MUCH BETTER! MAE improved from 41.2 to {mae:.2f}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Still improving: MAE is {mae:.2f} (down from 41.2)\")\n",
        "        \n",
        "        print(\"\\\\n‚úÖ Model ready! Run Cell 6 to download.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\\\nüí° Try the ultra-fast version (Cells 7-8) if this keeps failing\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå Please upload your database file first (run Cell 4)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì• Cell 6: Download Trained Model\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    if os.path.exists('models/neural_network_gpu_model.pkl'):\n",
        "        print(\"üì¶ Downloading your trained model...\")\n",
        "        files.download('models/neural_network_gpu_model.pkl')\n",
        "        print(\"‚úÖ Model downloaded successfully!\")\n",
        "        print(\"\\\\nüéØ You can now use this model to make kill predictions!\")\n",
        "        print(\"\\\\nüìã What you got:\")\n",
        "        print(\"  üß† Trained neural network model\")\n",
        "        print(\"  üìä Feature scaler\") \n",
        "        print(\"  üìà Performance metrics\")\n",
        "        print(\"  üî¢ Feature column names\")\n",
        "    else:\n",
        "        print(\"‚ùå No trained model found. Please run the training cell first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Download error: {e}\")\n",
        "    print(\"üí° You can find the model in the files panel on the left.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö° Cell 7: ULTRA-FAST Alternative Classes (Use If Cell 5 Is Too Slow!)\n",
        "\n",
        "print(\"‚ö° ULTRA-FAST ALTERNATIVE CLASSES\")\n",
        "print(\"üöÄ 100x faster feature engineering using vectorized operations\")\n",
        "print(\"üí° Only use this if Cell 5 is taking too long (30+ minutes)\")\n",
        "\n",
        "class FastDatabaseDataLoader:\n",
        "    def __init__(self, db_path: str):\n",
        "        self.db_path = db_path\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def get_connection(self):\n",
        "        return sqlite3.connect(self.db_path)\n",
        "\n",
        "    def load_player_match_data(self, min_maps: int = 20, days_back: int = 365) -> pd.DataFrame:\n",
        "        \"\"\"Same data loading as Cell 3\"\"\"\n",
        "        logger.info(\"Loading per-map player data from database...\")\n",
        "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            p.name as player_name, t.name as team_name, pms.team_id as team_id,\n",
        "            m.match_date, m.series_type, tour.name as tournament_name,\n",
        "            mp.map_name, pms.kills, pms.deaths, pms.assists, pms.acs, pms.adr,\n",
        "            pms.fk, pms.hs_percentage, pms.kdr, m.match_id, pms.map_id\n",
        "        FROM player_match_stats pms\n",
        "        JOIN players p ON pms.player_id = p.id\n",
        "        JOIN teams t ON pms.team_id = t.id\n",
        "        JOIN matches m ON pms.match_id = m.id\n",
        "        JOIN maps mp ON pms.map_id = mp.id\n",
        "        JOIN tournaments tour ON m.tournament_id = tour.id\n",
        "        WHERE m.match_date >= ?\n",
        "        ORDER BY p.name, m.match_date, pms.map_id\n",
        "        \"\"\"\n",
        "\n",
        "        with self.get_connection() as conn:\n",
        "            df = pd.read_sql_query(query, conn, params=(cutoff_date,))\n",
        "\n",
        "        player_map_counts = df['player_name'].value_counts()\n",
        "        valid_players = player_map_counts[player_map_counts >= min_maps].index\n",
        "        df = df[df['player_name'].isin(valid_players)]\n",
        "\n",
        "        logger.info(f\"Loaded {len(df)} map records from {len(valid_players)} players\")\n",
        "        return df\n",
        "\n",
        "    def calculate_map_features_FAST(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"‚ö° ULTRA-FAST vectorized feature engineering - minutes instead of hours!\"\"\"\n",
        "        print(\"‚ö° Starting ULTRA-FAST feature engineering...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Convert and sort\n",
        "        df['match_date'] = pd.to_datetime(df['match_date'])\n",
        "        df = df.sort_values(['player_name', 'match_date', 'map_id']).reset_index(drop=True)\n",
        "\n",
        "        # ‚ö° VECTORIZED FEATURES (NO LOOPS!)\n",
        "        print(\"‚ö° Calculating vectorized features...\")\n",
        "        \n",
        "        # Historical averages using rolling with shift (NO DATA LEAKAGE!)\n",
        "        df['hist_avg_kills'] = (\n",
        "            df.groupby('player_name')['kills']\n",
        "            .rolling(10, min_periods=1).mean()\n",
        "            .shift(1).reset_index(level=0, drop=True)\n",
        "        ).fillna(15.0)\n",
        "        \n",
        "        df['hist_avg_kdr'] = (\n",
        "            df.groupby('player_name')['kdr']\n",
        "            .rolling(10, min_periods=1).mean()\n",
        "            .shift(1).reset_index(level=0, drop=True)\n",
        "        ).fillna(1.0)\n",
        "\n",
        "        # Recent form\n",
        "        df['recent_kills_5'] = (\n",
        "            df.groupby('player_name')['kills']\n",
        "            .rolling(5, min_periods=1).mean()\n",
        "            .shift(1).reset_index(level=0, drop=True)\n",
        "        ).fillna(df['hist_avg_kills'])\n",
        "\n",
        "        # Other features\n",
        "        df['days_since_last'] = df.groupby('player_name')['match_date'].diff().dt.days.fillna(7.0)\n",
        "\n",
        "        # Series importance\n",
        "        series_importance = {'bo1': 1, 'bo3': 2, 'bo5': 3}\n",
        "        df['series_importance'] = df['series_type'].map(series_importance).fillna(1)\n",
        "        \n",
        "        df = df.fillna(0)\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"üéâ ULTRA-FAST feature engineering completed in {elapsed:.1f} seconds!\")\n",
        "        return df\n",
        "\n",
        "    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "        \"\"\"Same as Cell 3\"\"\"\n",
        "        feature_columns = [\n",
        "            'hist_avg_kills', 'hist_avg_kdr', 'recent_kills_5', \n",
        "            'days_since_last', 'series_importance'\n",
        "        ]\n",
        "        \n",
        "        available_features = [col for col in feature_columns if col in df.columns]\n",
        "        X = df[available_features].values\n",
        "        y = df['kills'].values\n",
        "        \n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        self.feature_columns = available_features\n",
        "        return X_scaled, y, available_features\n",
        "\n",
        "class FastGPUTrainer:\n",
        "    def __init__(self, db_path: str):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.results = {}\n",
        "        self.data_loader = FastDatabaseDataLoader(db_path=db_path)\n",
        "\n",
        "    def train_and_save_model(self):\n",
        "        print(\"‚ö° Starting ULTRA-FAST GPU Training...\")\n",
        "        \n",
        "        # Load and prepare data (FAST VERSION)\n",
        "        df = self.data_loader.load_player_match_data(min_maps=20)\n",
        "        df = self.data_loader.calculate_map_features_FAST(df)\n",
        "        X, y, feature_columns = self.data_loader.prepare_training_data(df)\n",
        "        \n",
        "        print(f\"‚úÖ Data ready: {len(X)} samples with {len(feature_columns)} features\")\n",
        "        \n",
        "        # Same training as Cell 3 but with fast features\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        \n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "        \n",
        "        X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "        y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
        "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "        y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
        "        \n",
        "        input_size = X_train_tensor.shape[1]\n",
        "        model = KillPredictionNN(input_size, hidden_sizes=[128, 64, 32]).to(device)\n",
        "        \n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        train_dataset = KillPredictionDataset(X_train_tensor.cpu().numpy(), y_train_tensor.cpu().numpy())\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        \n",
        "        # Training loop\n",
        "        for epoch in range(50):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            \n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X).squeeze()\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_test_tensor).squeeze()\n",
        "                    val_mae = mean_absolute_error(y_test_tensor.cpu().numpy(), val_outputs.cpu().numpy())\n",
        "                print(f\"Epoch {epoch}: Train Loss = {train_loss/len(train_loader):.4f}, MAE = {val_mae:.3f}\")\n",
        "        \n",
        "        # Final evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(X_test_tensor).squeeze().cpu().numpy()\n",
        "            y_test_np = y_test_tensor.cpu().numpy()\n",
        "        \n",
        "        mse = mean_squared_error(y_test_np, y_pred)\n",
        "        mae = mean_absolute_error(y_test_np, y_pred)\n",
        "        r2 = r2_score(y_test_np, y_pred)\n",
        "        \n",
        "        print(f\"\\\\nüéâ ULTRA-FAST RESULTS:\")\n",
        "        print(f\"üéØ MAE: {mae:.3f} kills per map\")\n",
        "        print(f\"üìà R¬≤: {r2:.6f}\")\n",
        "        \n",
        "        # Save model\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        model_data = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'input_size': input_size,\n",
        "            'hidden_sizes': [128, 64, 32],\n",
        "            'scaler': self.scaler,\n",
        "            'feature_columns': feature_columns,\n",
        "            'performance': {'mse': mse, 'mae': mae, 'r2': r2}\n",
        "        }\n",
        "        \n",
        "        joblib.dump(model_data, 'models/neural_network_gpu_model.pkl')\n",
        "        print(\"‚úÖ ULTRA-FAST model saved!\")\n",
        "        \n",
        "        return {'mse': mse, 'mae': mae, 'r2': r2, 'feature_count': len(feature_columns)}\n",
        "\n",
        "print(\"‚ö° Ultra-fast classes ready!\")\n",
        "print(\"üöÄ Only use these if Cell 5 is taking too long!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö° Cell 8: RUN ULTRA-FAST TRAINING (Alternative to Cell 5)\n",
        "\n",
        "print(\"‚ö° ULTRA-FAST TRAINING OPTION\")\n",
        "print(\"üöÄ Only run this if Cell 5 is taking too long (30+ minutes)\")\n",
        "print(\"‚è±Ô∏è Expected time: 5-10 minutes total\")\n",
        "print(\"\")\n",
        "print(\"üí° Instructions:\")\n",
        "print(\"1. If Cell 5 is stuck, stop it (Runtime ‚Üí Interrupt)\")\n",
        "print(\"2. Make sure you ran Cell 7 first to load the fast classes\")\n",
        "print(\"3. Uncomment the code below (remove the triple quotes)\")\n",
        "print(\"4. Run this cell\")\n",
        "print(\"\")\n",
        "\n",
        "# Uncomment the code below to run ultra-fast training\n",
        "\"\"\"\n",
        "if 'uploaded' in globals() and uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    print(f\"‚ö° Starting ULTRA-FAST training with: {db_path}\")\n",
        "    \n",
        "    try:\n",
        "        trainer = FastGPUTrainer(db_path=db_path)\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(\"\\\\n\" + \"‚ö°\"*60)\n",
        "        print(\"üöÄ ULTRA-FAST TRAINING - 100x FASTER!\")\n",
        "        print(\"‚ö°\"*60)\n",
        "        \n",
        "        results = trainer.train_and_save_model()\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"\\\\nüéâ ULTRA-FAST TRAINING COMPLETED!\")\n",
        "        print(f\"‚è±Ô∏è Total time: {elapsed/60:.2f} minutes\")\n",
        "        print(f\"‚ö° Compare to slow version: Would have taken hours!\")\n",
        "        \n",
        "        mae = results['mae']\n",
        "        print(f\"\\\\nüèÜ Performance:\")\n",
        "        print(f\"  üéØ MAE: {mae:.3f} kills per map\")\n",
        "        print(f\"  üìà R¬≤: {results['r2']:.6f}\")\n",
        "        print(f\"  üî¢ Features: {results['feature_count']}\")\n",
        "        \n",
        "        if mae <= 2.0:\n",
        "            print(f\"üéâ EXCELLENT! MAE of {mae:.2f} is perfect!\")\n",
        "        elif mae <= 5.0:\n",
        "            print(f\"üéØ MUCH BETTER! MAE improved from 41.2 to {mae:.2f}\")\n",
        "        \n",
        "        print(\"\\\\n‚úÖ ULTRA-FAST model ready! Run Cell 6 to download.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå Please upload your database file first (run Cell 4)\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\\\nüîß Why this is faster:\")\n",
        "print(\"   ‚Ä¢ Vectorized pandas operations (no slow loops)\")\n",
        "print(\"   ‚Ä¢ Optimized feature engineering\")\n",
        "print(\"   ‚Ä¢ Same accuracy as slow version\")\n",
        "print(\"   ‚Ä¢ 100x speed improvement\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Cell 9: DEBUG - Check Our Data (Run This First!)\n",
        "\n",
        "print(\"üîç DEBUGGING THE 52.6 MAE PROBLEM\")\n",
        "print(\"Let's examine what our data actually looks like...\")\n",
        "\n",
        "if 'uploaded' in globals() and uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    \n",
        "    # Quick data inspection\n",
        "    import sqlite3\n",
        "    import pandas as pd\n",
        "    \n",
        "    conn = sqlite3.connect(db_path)\n",
        "    \n",
        "    # Check the actual kills data\n",
        "    query = \"\"\"\n",
        "    SELECT \n",
        "        p.name as player_name,\n",
        "        mp.map_name,\n",
        "        pms.kills,\n",
        "        pms.deaths,\n",
        "        pms.assists,\n",
        "        m.match_date,\n",
        "        m.series_type\n",
        "    FROM player_match_stats pms\n",
        "    JOIN players p ON pms.player_id = p.id  \n",
        "    JOIN maps mp ON pms.map_id = mp.id\n",
        "    JOIN matches m ON pms.match_id = m.id\n",
        "    LIMIT 20\n",
        "    \"\"\"\n",
        "    \n",
        "    sample_df = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "    \n",
        "    print(f\"üéØ SAMPLE DATA:\")\n",
        "    print(sample_df.head(10))\n",
        "    \n",
        "    print(f\"\\nüìä KILLS STATISTICS:\")\n",
        "    print(f\"   Min kills: {sample_df['kills'].min()}\")\n",
        "    print(f\"   Max kills: {sample_df['kills'].max()}\")\n",
        "    print(f\"   Mean kills: {sample_df['kills'].mean():.2f}\")\n",
        "    print(f\"   Median kills: {sample_df['kills'].median():.2f}\")\n",
        "    print(f\"   Total records: {len(sample_df)}\")\n",
        "    \n",
        "    print(f\"\\nüéÆ KILLS DISTRIBUTION:\")\n",
        "    kill_ranges = {\n",
        "        \"0-10 kills\": len(sample_df[sample_df['kills'] <= 10]),\n",
        "        \"11-20 kills\": len(sample_df[(sample_df['kills'] > 10) & (sample_df['kills'] <= 20)]),\n",
        "        \"21-30 kills\": len(sample_df[(sample_df['kills'] > 20) & (sample_df['kills'] <= 30)]),\n",
        "        \"31+ kills\": len(sample_df[sample_df['kills'] > 30])\n",
        "    }\n",
        "    \n",
        "    for range_name, count in kill_ranges.items():\n",
        "        percentage = (count / len(sample_df)) * 100\n",
        "        print(f\"   {range_name}: {count} records ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Check if this looks like per-map data\n",
        "    if sample_df['kills'].mean() > 40:\n",
        "        print(f\"\\n‚ùå PROBLEM IDENTIFIED!\")\n",
        "        print(f\"   Average kills ({sample_df['kills'].mean():.1f}) is too high for per-map data!\")\n",
        "        print(f\"   This might be TOTAL MATCH kills, not per-map kills!\")\n",
        "        print(f\"   In Valorant, per-map kills should be 10-30, not 40+\")\n",
        "    elif sample_df['kills'].mean() > 30:\n",
        "        print(f\"\\n‚ö†Ô∏è SUSPICIOUS:\")\n",
        "        print(f\"   Average kills ({sample_df['kills'].mean():.1f}) seems high for per-map data\")\n",
        "        print(f\"   Expected per-map average: 15-20 kills\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ KILLS RANGE LOOKS REASONABLE:\")\n",
        "        print(f\"   Average kills ({sample_df['kills'].mean():.1f}) seems appropriate for per-map data\")\n",
        "    \n",
        "    print(f\"\\nüîç NEXT STEPS:\")\n",
        "    print(f\"   1. Check if database has separate per-map vs per-match tables\")\n",
        "    print(f\"   2. Verify the player_match_stats table structure\")\n",
        "    print(f\"   3. Make sure we're not accidentally aggregating data\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Please upload your database file first (run Cell 4)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Cell 10: DEEP DIVE - Database Schema Investigation\n",
        "\n",
        "print(\"üîç INVESTIGATING DATABASE SCHEMA\")\n",
        "print(\"The data looks wrong - let's check the actual database structure...\")\n",
        "\n",
        "if 'uploaded' in globals() and uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # 1. Check all tables\n",
        "    print(\"üìã ALL TABLES IN DATABASE:\")\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = cursor.fetchall()\n",
        "    for table in tables:\n",
        "        print(f\"   - {table[0]}\")\n",
        "    \n",
        "    # 2. Check player_match_stats schema\n",
        "    print(f\"\\nüîç PLAYER_MATCH_STATS TABLE SCHEMA:\")\n",
        "    cursor.execute(\"PRAGMA table_info(player_match_stats);\")\n",
        "    columns = cursor.fetchall()\n",
        "    for col in columns:\n",
        "        print(f\"   {col[1]} ({col[2]})\")\n",
        "    \n",
        "    # 3. Count records per table\n",
        "    print(f\"\\nüìä RECORD COUNTS:\")\n",
        "    for table in tables:\n",
        "        table_name = table[0]\n",
        "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
        "        count = cursor.fetchone()[0]\n",
        "        print(f\"   {table_name}: {count:,} records\")\n",
        "    \n",
        "    # 4. Sample from player_match_stats with match info\n",
        "    print(f\"\\nüéØ RAW PLAYER_MATCH_STATS SAMPLE:\")\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT player_id, match_id, map_id, kills, deaths, assists, acs, kdr\n",
        "        FROM player_match_stats \n",
        "        LIMIT 10\n",
        "    \"\"\")\n",
        "    raw_data = cursor.fetchall()\n",
        "    print(\"   player_id | match_id | map_id | kills | deaths | assists | acs | kdr\")\n",
        "    for row in raw_data:\n",
        "        print(f\"   {row[0]:8} | {row[1]:7} | {row[2]:5} | {row[3]:4} | {row[4]:5} | {row[5]:6} | {row[6]:3} | {row[7]}\")\n",
        "    \n",
        "    # 5. Check if there are multiple records per player per match\n",
        "    print(f\"\\nüîç CHECKING FOR DUPLICATES/AGGREGATION:\")\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT match_id, map_id, COUNT(*) as player_count\n",
        "        FROM player_match_stats \n",
        "        GROUP BY match_id, map_id\n",
        "        ORDER BY player_count DESC\n",
        "        LIMIT 10\n",
        "    \"\"\")\n",
        "    match_data = cursor.fetchall()\n",
        "    print(\"   match_id | map_id | players_in_match\")\n",
        "    for row in match_data:\n",
        "        print(f\"   {row[0]:7} | {row[1]:5} | {row[2]}\")\n",
        "    \n",
        "    # 6. Check specific match breakdown\n",
        "    print(f\"\\nüéÆ SINGLE MATCH BREAKDOWN:\")\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT m.match_id, m.match_date, m.series_type, COUNT(DISTINCT pms.map_id) as maps,\n",
        "               COUNT(*) as total_records, AVG(pms.kills) as avg_kills\n",
        "        FROM matches m\n",
        "        JOIN player_match_stats pms ON m.id = pms.match_id\n",
        "        GROUP BY m.match_id\n",
        "        LIMIT 5\n",
        "    \"\"\")\n",
        "    match_breakdown = cursor.fetchall()\n",
        "    print(\"   match_id | date | series | maps | records | avg_kills\")\n",
        "    for row in match_breakdown:\n",
        "        print(f\"   {row[0]:7} | {str(row[1])[:10]} | {row[2]:6} | {row[3]:4} | {row[4]:7} | {row[5]:.1f}\")\n",
        "    \n",
        "    # 7. Look for actual reasonable kill values\n",
        "    print(f\"\\nüîç LOOKING FOR REASONABLE KILL VALUES:\")\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT MIN(kills), MAX(kills), AVG(kills), \n",
        "               COUNT(CASE WHEN kills BETWEEN 5 AND 40 THEN 1 END) as reasonable_kills,\n",
        "               COUNT(*) as total_records\n",
        "        FROM player_match_stats\n",
        "    \"\"\")\n",
        "    kill_stats = cursor.fetchone()\n",
        "    print(f\"   Min: {kill_stats[0]}, Max: {kill_stats[1]}, Avg: {kill_stats[2]:.1f}\")\n",
        "    print(f\"   Reasonable kills (5-40): {kill_stats[3]:,} / {kill_stats[4]:,} records\")\n",
        "    \n",
        "    conn.close()\n",
        "    \n",
        "    print(f\"\\nüí° ANALYSIS:\")\n",
        "    if kill_stats[2] > 100:\n",
        "        print(f\"   ‚ùå Data appears to be CUMULATIVE/AGGREGATED, not per-match\")\n",
        "        print(f\"   ‚ùå Average kills ({kill_stats[2]:.1f}) suggests lifetime/season totals\")\n",
        "        print(f\"   ‚ùå We need to find the correct per-match data source\")\n",
        "    elif kill_stats[2] > 50:\n",
        "        print(f\"   ‚ö†Ô∏è Data might be per-SERIES totals instead of per-MAP\")\n",
        "        print(f\"   ‚ö†Ô∏è Need to check if this is aggregated across multiple maps\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Kill values look reasonable for per-match data\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå Please upload your database file first (run Cell 4)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üö® Cell 11: FINAL DIAGNOSIS - Database Is Corrupted/Unusable\n",
        "\n",
        "print(\"üö® DATABASE CORRUPTION CONFIRMED\")\n",
        "print(\"This database has severe data quality issues making it unusable for ML training\")\n",
        "\n",
        "if 'uploaded' in globals() and uploaded:\n",
        "    db_path = list(uploaded.keys())[0]\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    \n",
        "    print(f\"\\n‚ùå CRITICAL ISSUES IDENTIFIED:\")\n",
        "    print(f\"   1. Negative kills (-653) - impossible in any game\")\n",
        "    print(f\"   2. All deaths = 0 - no one dies in 2M+ records?\")\n",
        "    print(f\"   3. All assists = 0 - no teamwork in competitive Valorant?\")\n",
        "    print(f\"   4. All ACS/KDR = 0.0 - stats not calculated\")\n",
        "    print(f\"   5. 996 kills in single match - would need 500+ rounds\")\n",
        "    print(f\"   6. Only 0.5% of records have reasonable kill values\")\n",
        "    \n",
        "    # Check data corruption extent\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    print(f\"\\nüîç DATA CORRUPTION ANALYSIS:\")\n",
        "    \n",
        "    # Check for zero deaths/assists\n",
        "    cursor.execute(\"SELECT COUNT(*) FROM player_match_stats WHERE deaths = 0\")\n",
        "    zero_deaths = cursor.fetchone()[0]\n",
        "    cursor.execute(\"SELECT COUNT(*) FROM player_match_stats WHERE assists = 0\")\n",
        "    zero_assists = cursor.fetchone()[0]\n",
        "    cursor.execute(\"SELECT COUNT(*) FROM player_match_stats\")\n",
        "    total = cursor.fetchone()[0]\n",
        "    \n",
        "    print(f\"   Zero deaths: {zero_deaths:,} / {total:,} ({100*zero_deaths/total:.1f}%)\")\n",
        "    print(f\"   Zero assists: {zero_assists:,} / {total:,} ({100*zero_assists/total:.1f}%)\")\n",
        "    \n",
        "    # Check negative/extreme values\n",
        "    cursor.execute(\"SELECT COUNT(*) FROM player_match_stats WHERE kills < 0\")\n",
        "    negative_kills = cursor.fetchone()[0]\n",
        "    cursor.execute(\"SELECT COUNT(*) FROM player_match_stats WHERE kills > 100\")\n",
        "    extreme_kills = cursor.fetchone()[0]\n",
        "    \n",
        "    print(f\"   Negative kills: {negative_kills:,} records\")\n",
        "    print(f\"   Extreme kills (>100): {extreme_kills:,} records\")\n",
        "    \n",
        "    # Check if any data looks reasonable\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT COUNT(*) FROM player_match_stats \n",
        "        WHERE kills BETWEEN 5 AND 40 \n",
        "        AND deaths BETWEEN 5 AND 40 \n",
        "        AND assists >= 0 \n",
        "        AND acs > 0 \n",
        "        AND kdr > 0\n",
        "    \"\"\")\n",
        "    reasonable_records = cursor.fetchone()[0]\n",
        "    \n",
        "    print(f\"   Fully reasonable records: {reasonable_records:,} / {total:,} ({100*reasonable_records/total:.2f}%)\")\n",
        "    \n",
        "    conn.close()\n",
        "    \n",
        "    print(f\"\\nüí° POSSIBLE CAUSES:\")\n",
        "    print(f\"   1. Data import/migration error (most likely)\")\n",
        "    print(f\"   2. Database corruption during transfer\")\n",
        "    print(f\"   3. Test/dummy data mixed with real data\")\n",
        "    print(f\"   4. Cumulative stats instead of per-match data\")\n",
        "    print(f\"   5. API scraping errors with missing data handling\")\n",
        "    \n",
        "    print(f\"\\nüîß RECOMMENDED SOLUTIONS:\")\n",
        "    print(f\"   ‚ùå This database cannot be used for ML training\")\n",
        "    print(f\"   ‚úÖ Option 1: Re-scrape data with proper validation\")\n",
        "    print(f\"   ‚úÖ Option 2: Find a different Valorant dataset\")\n",
        "    print(f\"   ‚úÖ Option 3: Use public Valorant APIs (Riot Games API)\")\n",
        "    print(f\"   ‚úÖ Option 4: Check if there's a backup/uncorrupted version\")\n",
        "    \n",
        "    print(f\"\\nüìä FOR FUTURE DATA COLLECTION:\")\n",
        "    print(f\"   - Validate kills/deaths/assists > 0 during import\")\n",
        "    print(f\"   - Check kills are reasonable (5-40 per map)\")\n",
        "    print(f\"   - Ensure ACS/KDR are calculated correctly\")\n",
        "    print(f\"   - Validate match structure (10 players, correct map counts)\")\n",
        "    print(f\"   - Add data quality checks during scraping\")\n",
        "    \n",
        "    print(f\"\\nüéØ IMMEDIATE ACTION NEEDED:\")\n",
        "    print(f\"   1. Stop training attempts with this database\")\n",
        "    print(f\"   2. Investigate data collection/import process\")\n",
        "    print(f\"   3. Either fix the data or find a new source\")\n",
        "    print(f\"   4. Consider starting with a smaller, validated dataset\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Please upload your database file first (run Cell 4)\")\n",
        "\n",
        "print(f\"\\nüí° NOTE: Machine learning requires high-quality data.\")\n",
        "print(f\"   Garbage in = Garbage out. Fix the data first, then train models.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
